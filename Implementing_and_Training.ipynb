{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing and Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOx7SxVn-cJB"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = .001\n",
        "\n",
        "\n",
        "class FeedForwardNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dense_layer = nn.Sequential(\n",
        "        nn.Linear(28*28, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)\n",
        "    )\n",
        "    self.softmac = nn.Softmax(dim = 1)\n",
        "\n",
        "  \n",
        "  def forward (self, input_data):\n",
        "    flatten_data = self.flatten(input_data)\n",
        "    logits = self.dense_layer(flattened_data)\n",
        "    predictions = self.Softmax(logits)\n",
        "    return predictions\n",
        "\n",
        "from torchvision import transforms\n",
        "def download_minst_datasets():\n",
        "  train_data = datasets.MNIST(\n",
        "      root = \"data\",\n",
        "      download = True,\n",
        "      train = True,\n",
        "      transform = ToTensor()\n",
        "  )\n",
        "\n",
        "  validation_data = datasets.MNIST(\n",
        "      root = \"data\",\n",
        "      download = True,\n",
        "      train = False,\n",
        "      transform = ToTensor()\n",
        "  )\n",
        "\n",
        "  return train_data, validation_data\n",
        "\n",
        "train_data, _ = download_minst_datasets()\n",
        "print(\"MNIST dataset download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TEYYNZnLsY-",
        "outputId": "aa583ca6-2435-4c48-8012-84477c572e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "feed_forward_net = FeedForwardNet().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeQaojfyQocL",
        "outputId": "ffc995a9-234c-4abb-eae7-2c1b29e1d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "  for inputes, targets in data_loader:\n",
        "    inputs, targets = inputs. to(device), targets.to(device)\n",
        "    predictations = model(inputs)\n",
        "    loss = loss_fn(predictations, targets)\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.stpe()\n",
        "\n",
        "  print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "  for i in range(epochs):\n",
        "    print(f\"Epoch{i+1}\")\n",
        "    train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "    print(\"=================\")\n",
        "  print(\"Trainning is done\")\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(feed_forward_net.parameters(),lr = LEARNING_RATE)\n",
        "  \n",
        "  train(feed_forward_net,train_data_loader,loss_fn,optimiser,device,EPOCHS)\n",
        "\n",
        "  torch.save(feed_forward_net.state_dict(), \"feeddorwardnet.pth\")\n",
        "  print(\"model trained and stored at feetforwardnet.pth\")"
      ],
      "metadata": {
        "id": "FFZSPDteYBhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}