{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainsound.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLMdAiZWBmoh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from urbansounddataset import UrbanSoundDataset\n",
        "from cnn import CNNNetwork"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "ANNOTATIONS_FILE = \n",
        "AUDIO_DIR = \n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n"
      ],
      "metadata": {
        "id": "xg6XYdZtBnys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(train_data, batch_size):\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for input, target in data_loader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        # calculate loss\n",
        "        prediction = model(input)\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # backpropagate error and update weights\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print(f\"loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")"
      ],
      "metadata": {
        "id": "-5o1zgIJBsUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        n_mels=64\n",
        "    )\n",
        "\n",
        "    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n",
        "                            AUDIO_DIR,\n",
        "                            mel_spectrogram,\n",
        "                            SAMPLE_RATE,\n",
        "                            NUM_SAMPLES,\n",
        "                            device)\n",
        "    \n",
        "    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n",
        "\n",
        "    cnn = CNNNetwork().to(device)\n",
        "    print(cnn)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(cnn.parameters(),\n",
        "                                 lr=LEARNING_RATE)\n",
        "\n",
        "   \n",
        "    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "    \n",
        "    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
        "    print(\"Trained feed forward net saved at feedforwardnet.pth\")"
      ],
      "metadata": {
        "id": "rP1gO3vzBwdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}